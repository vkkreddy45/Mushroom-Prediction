
from distutils.log import Log
from enum import unique

from pickle import FALSE, NONE, TRUE
from re import I, S
from tkinter import Y
import pandas as pd
import numpy as np
from numpy import mean
from numpy import absolute
from collections import Counter
from sklearn.preprocessing import StandardScaler


from sklearn.model_selection import train_test_split




mush_data=pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data",header=None)



c=["class","cap-shape","cap-surface","cap-color","bruises","odor","gill-attachment","gill-spacing","gill-size"	,"gill-color",	"stalk-shape",	"stalk-root"	,"stalk-surface-above-ring"	,"stalk-surface-below-ring",	"stalk-color-above-ring",	"stalk-color-below-ring",	"veil-type"	,"veil-color",	"ring-number"	,"ring-type"	,"spore-print-color"	,"population"	,"habitat"]

mush_data.columns=c

mode = mush_data[mush_data["stalk-root"]!="?"]["stalk-root"].mode()[0]
#print(mode)
mush_data["stalk-root"] = mush_data["stalk-root"].replace("?", mode)

mush_data=mush_data.drop(labels=['veil-type'],axis=1)

def normalization(data,col_name):
    min_value=data[col_name].min()
    max_value=data[col_name].max()
    range_value=max_value-min_value
    for i in range(len(data)):
        val=data.iloc[i][col_name]
        data[col_name][i]=float((val-min_value)/range_value)
    uniq=np.unique(data[col_name].tolist())
    #print(list(uniq))
    return data,list(uniq)

def labelling(data,col_name):
    uniq=data[col_name].unique()
    for i in range(len(data)):
        for  j in range(len(uniq)):
            if(data[col_name][i]==uniq[j]):
                data[col_name][i]=int(j)
    
    return data,uniq


c=["class","cap-shape","cap-surface","cap-color","bruises","odor","gill-attachment","gill-spacing","gill-size","gill-color",	"stalk-shape",	"stalk-root"	,"stalk-surface-above-ring"	,"stalk-surface-below-ring",	"stalk-color-above-ring",	"stalk-color-below-ring","veil-color",	"ring-number"	,"ring-type"	,"spore-print-color"	,"population"	,"habitat"]

# ss=[]
# for k in c:
#     label_data,uniq=labelling(mush_data,k)
#     mush_data=label_data
#     ss.append(uniq)



# for k in c:
#     mush_data[k]=mush_data[k].astype(float)


# for i in range(len(ss)):
#     ss[i]=list(ss[i])

# ss.remove(ss[0])  

# ww=[]
# for k in c:
#     uniq=mush_data[k].unique()

#     norm_data,uniq=normalization(mush_data,k)
#     mush_data=norm_data
#     ww.append(uniq)

# for i in range(len(ww)):
#     ww[i]=list(ww[i])

# ww.remove(ww[0]) 

# target=mush_data["class"]
# #print(target)
# mush_data=mush_data.drop(labels="class",axis=1)
# target=target.astype('int32')
# print(mush_data)





class LogisticReg():
    
    # defining parameters such as learning rate, number ot iterations, whether to include intercept, 
    # and verbose which says whether to print anything or not like, loss etc.
    def __init__(self, learning_rate=0.01, num_iterations=50000, fit_intercept=True, verbose=False):
        self.learning_rate = learning_rate
        self.num_iterations = num_iterations
        self.fit_intercept = fit_intercept
        self.verbose = verbose
    
    
    # this is the function which trains our model.
    def fit(self, X, y):
        
        # as said if we want our intercept term to be added we use fit_intercept=True
        if self.fit_intercept:
            intercept = np.ones((X.shape[0], 1))
            X = np.concatenate((intercept, X), axis=1)
        
        # weights initialization of our Normal Vector, initially we set it to 0, then we learn it eventually
        self.W = np.zeros(X.shape[1])
        
        # this for loop runs for the number of iterations provided
        for i in range(self.num_iterations):
            
            # this is our W * Xi
            z = np.dot(X, self.W)
            
            # this is where we predict the values of Y based on W and Xi
            yp = 1 / (1 + np.exp(-z))
            
            # this is where the gradient is calculated form the error generated by our model
            gradient = np.dot(X.T, (yp - y)) / y.size
            
            # this is where we update our values of W, so that we can use the new values for the next iteration
            self.W -= self.learning_rate * gradient
            
            # this is our new W * Xi
            z = np.dot(X, self.W)
            yp = 1 / (1 + np.exp(-z))
            
            # this is where the loss is calculated
            loss = (-y * np.log(yp) - (1 - y) * np.log(1 - yp)).mean()
            
            # as mentioned above if we want to print somehting we use verbose, so if verbose=True then our loss get printed
            if(self.verbose ==True and i % 10000 == 0):
                print(f'loss: {loss} \t')
    
    # this is where we predict the probability values based on out generated W values out of all those iterations.
    def predict(self, X):
        # as said if we want our intercept term to be added we use fit_intercept=True
        if self.fit_intercept:
            intercept = np.ones((X.shape[0], 1))
            X = np.concatenate((intercept, X), axis=1)
        
        pred=1 / (1 + np.exp(-np.dot(X, self.W)))
        # this is the final prediction that is generated based on the values learned.
        return pred.round()
        



def entropy(y):
    h = np.bincount(y)
    t = h / len(y)
    return -np.sum([p * np.log2(p) for p in t if p > 0])

from collections import Counter

class Node:
    def __init__(
        self, feature=None, threshold=None, left=None, right=None, *, value=None
    ):
        self.feature = feature
        self.threshold = threshold
        self.left = left
        self.right = right
        self.value = value

    def is_leaf_node(self):
        return self.value is not None
    


class DecisionTree:
    def __init__(self, min_samples_split=2, max_depth=100, n_feats=None):
        self.min_samples_split = min_samples_split
        self.max_depth = max_depth
        self.n_feats = n_feats
        self.root = None

    def fit(self, X, y):
        self.n_feats = X.shape[1] if not self.n_feats else min(self.n_feats, X.shape[1])
        self.root = self.grow_tree(X, y)
    
    def predict(self, X):
        return np.array([self.traverse(x, self.root) for x in X])

    
    def grow_tree(self, X, y, depth=0):
        n_samples, n_features = X.shape
        n_labels = len(np.unique(y))

        # stopping criteria
        if (
            depth >= self.max_depth
            or n_labels == 1
            or n_samples < self.min_samples_split
        ):
            counter = Counter(y)
            leaf_value = counter.most_common(1)[0][0]
            return Node(value=leaf_value)

        f_ids = np.random.choice(n_features, self.n_feats, replace=False)

        # greedily select the best split according to information gain
        best_feat, best_thresh = self.best_criteria(X, y, f_ids)

        # grow the children that result from the split
        lids = np.argwhere(X[:, best_feat] <= best_thresh).flatten()
        rids = np.argwhere(X[:, best_feat] > best_thresh).flatten()
        #         l_ids, r_ids = self._split(X[:, best_feat], best_thresh)
        left = self.grow_tree(X[lids, :], y[lids], depth + 1)
        right = self.grow_tree(X[rids, :], y[rids], depth + 1)
        return Node(best_feat, best_thresh, left, right)

    def best_criteria(self, X, y, f_ids):
        best_gain = -1
        split_idx, split_thresh = None, None
        for f_id in f_ids:
            X1 = X[:, f_id]
            thresholds = np.unique(X1)
            for t in thresholds:
                parent_entropy = entropy(y)

                # generate split
                lids = np.argwhere(X1 <= t).flatten()
                rids = np.argwhere(X1 > t).flatten()
                #l_ids, r_ids = self._split(X_column, split_thresh)
                if len(lids) == 0 or len(rids) == 0:
                    gain=0

                # compute the weighted avg. of the loss for the children
                n = len(y)
                n_l, n_r = len(lids), len(rids)
                e_l, e_r = entropy(y[lids]), entropy(y[rids])
                child_entropy = (n_l / n) * e_l + (n_r / n) * e_r

                # information gain is difference in loss before vs. after split
                gain= parent_entropy - child_entropy


                #gain = self._information_gain(y, X_column, threshold)

                if gain > best_gain:
                    best_gain = gain
                    split_idx = f_id
                    split_thresh = t

        return split_idx, split_thresh


    def traverse(self, x, node):
        if node.is_leaf_node():
            return node.value

        if x[node.feature] <= node.threshold:
            return self.traverse(x, node.left)
        return self.traverse(x, node.right)

def most_common_label(y):
    counter = Counter(y)
    most_common = counter.most_common(1)[0][0]
    return most_common
    

class RandomForest:
    def __init__(self, ntrees=10, minsamples_split=2, maxdepth=100, nfeats=None):
        self.ntrees = ntrees
        self.minsamples_split = minsamples_split
        self.maxdepth = maxdepth
        self.nfeats = nfeats
        self.trees = []

  
    def bootstrap_sample(self,X, y):
        n_samples = X.shape[0]
        idxs = np.random.choice(n_samples, n_samples, replace=True)
        return X[idxs], y[idxs]
    
    def fit(self, X, y):
        self.trees = []
        for _ in range(self.ntrees):
            tree = DecisionTree()
            n_samples = X.shape[0]
            idxs = np.random.choice(n_samples, n_samples, replace=True)
            X_samp=X[idxs]
            y_samp=y[idxs]
            tree.fit(X_samp, y_samp)
            self.trees.append(tree)

    def predict(self, X):
        preds = np.array([tree.predict(X) for tree in self.trees])
        preds = np.swapaxes(preds, 0, 1)
        y_pred = [most_common_label(pred) for pred in preds]
        return np.array(y_pred)




class KNN():
    def __init__(self,k=5, r=2):
        self.k=k
        self.r=r
        self.X_train=None
        self.y_train=None

    def fit(self,X_train,y_train):
        self.X_train=X_train
        self.y_train=y_train

    def predict(self, X_test):
        pr=self.knn_predict(self.X_train, X_test, self.y_train, self.k, self.r)
        return pr

    def minko_dis(self, p, q, r):
        # Set initial distance to 0
        dis=0
        # Calculate minkowski distance using parameter r
        for d in range(len(p)):
            dis+=abs(p[d] - q[d])**r
        dis = dis**(1/r)
        return dis

    def knn_predict(self, X_train, X_test, y_train, k, r):
        
        # Counter to help with label voting
        from collections import Counter
      
        # Make predictions on the test data
        # Need output of 1 prediction per test data point
        y_pre_tests = []

        for test_points in X_test:
            dist = []

            for train_points in X_train:
                distan = self.minko_dis(test_points, train_points, r=r)
                dist.append(distan)
            
            # Store distances in a dataframe
            df_dists = pd.DataFrame(data=dist, columns=['distance'], 
                                    index=y_train.index)
            
            # Sort distances, and only consider the k closest points
            df_R = df_dists.sort_values(by=['distance'], axis=0)[:k]

            # Create counter object to track the labels of k closest neighbors
            count = Counter(y_train[df_R.index])

            # Get most common label of all the nearest neighbors
            prediction = count.most_common()[0][0]
            
            # Append prediction to output list
            y_pre_tests.append(prediction)
            
        return y_pre_tests



rf=RandomForest()
knn=KNN(k=2, r=1)
lr=LogisticReg()


#X_train, X_test, y_train, y_test = train_test_split(mush_data, target, test_size=0.2, random_state=42)

#model=lr.fit(X_train,y_train)
#y_pred = lr.predict(X_test)
# model=rf.fit(np.array(X_train),np.array(y_train))
# y_pred = rf.predict(np.array(X_test))

# scaler = StandardScaler()
# X_train = scaler.fit_transform(X_train)
# X_test = scaler.transform(X_test)
# knn.fit(X_train, y_train)
# y_pred=knn.predict(X_test)


# print(y_pred)

def f_tp_tn_fn_fp(y_test, y_pred):

    tp = sum((y_test == 1) & (y_pred == 1))
    tn = sum((y_test == 0) & (y_pred == 0))
    fn = sum((y_test == 1) & (y_pred == 0))
    fp = sum((y_test == 0) & (y_pred == 1))
    return tp, tn, fp, fn


def f_accuracy(tp, tn, fn, fp):
	
	return ((tp + tn) * 100)/ (tp + tn + fn + fp)

def f_precision(tp, fp):
	
	return (tp  * 100)/  (tp + fp)

def f_recall(tp, fn):

	return (tp  * 100)/ tp + fn

def f1_score(y_test, y_pred):
    # calculates the F1 score
    tp, tn, fp, fn = f_tp_tn_fn_fp(y_test, y_pred)
    precision = f_precision(tp, fp)/100
    recall = f_recall(tp, fn)/100
    f1_score = (2*precision*recall)/ (precision + recall)
    return f1_score

# tp,tn,fp,fn=f_tp_tn_fn_fp(y_test,np.array(y_pred))
# print(tp,tn,fp,fn)
# acc=float(f_accuracy(tp, tn, fn, fp))
# pre=float(f_precision(tp, fp))
# recall=float(f_recall(tp, fn))
# f1score=float(f1_score(y_test, y_pred))

# print(acc,pre,recall,f1score)